# Attention SinkとResidual Sinkの統一的理解：外れ値駆動リスケーリングはTransformer学習に不可欠である

**2026年2月2日**

Zihan Qiu\*¹, Zeyu Huang\*², Kaiyue Wen\*³, Peng Jin\*¹, Bo Zheng\*¹,
Yuxin Zhou¹, Haofeng Huang¹⁴, Zekun Wang¹, Xiao Li¹, Huaqing Zhang¹⁴,
Yang Xu¹, Haoran Lian¹, Siqi Zhang¹, Rui Men¹, Jianwei Zhang¹,
Ivan Titov², Dayiheng Liu†¹, Jingren Zhou¹, Junyang Lin†¹

¹Qwen Team ²エディンバラ大学 ³スタンフォード大学 ⁴清華大学

\*同等貢献。†責任著者。

---

## 概要

本研究では、大規模言語モデル（LLM）において創発的に出現する外れ値の機能的役割を調査する。具体的には、**attention sink**（少数のトークンが一貫して大きなattentionロジットを受け取る現象）と**residual sink**（ほとんどのトークンにわたって少数の固定次元が持続的に大きな活性化値を示す現象）に焦点を当てる。我々は、これらの外れ値が対応する正規化（例：softmax attentionやRMSNorm）と連携して、他の非外れ値成分を効果的にリスケーリングしていると仮説を立てる。この現象を**外れ値駆動リスケーリング（outlier-driven rescaling）**と名付け、異なるモデルアーキテクチャおよび学習トークン数にわたってこの仮説を検証する。この視点は、両タイプのsinkの起源と軽減策を統一的に説明する。主な結論と観察は以下の通りである：

1. **外れ値は正規化と共同で機能する**：正規化を除去すると対応する外れ値は消失するが、学習の安定性と性能が低下する。正規化を保持したまま外れ値を直接クリッピングしても性能低下が生じ、外れ値駆動リスケーリングが学習の安定性に寄与していることを示す。
2. **外れ値は寄与者というよりリスケール因子として機能する**。attention sinkとresidual sinkの最終的な寄与は非外れ値のそれと比較して有意に小さい。
3. **外れ値は学習可能なパラメータに吸収するか、明示的なゲート付きリスケーリングにより軽減できる**。これにより学習性能の向上（平均2ポイントの改善）と量子化耐性の強化（W4A4量子化で1.2ポイントの性能低下）が実現される。

---

## 1 はじめに

Transformerベースの大規模言語モデル（LLM）は外れ値を示す。これらの極端な値は、通常の活性化値やロジットを桁違いに超え、実用上の課題をもたらす：モデル量子化時に表現のダイナミックレンジを支配し（Yao et al., 2022; Xiao et al., 2023b;a; Wei et al., 2023; Abecassis et al., 2025）、浮動小数点演算でより大きな数値誤差を引き起こす可能性がある（Budzinskiy et al., 2025）。しかし、クリッピングによって単純に除去するとモデル性能が著しく低下し（Kovaleva et al., 2021; Sun et al., 2024）、これらがTransformerにおいて不可欠な機能的役割を果たしていることを示唆している。

外れ値の顕著な例として**attention sink**（Xiao et al., 2023b）がある。これは、attentionロジットの小さなサブセットが他より大きくなり、少数の特殊トークン（sinkトークン）が一貫して高いattentionスコアを受け取る現象である。最近の研究は、その形成がsoftmax正規化と本質的に関連していることを明らかにし（Bondarenko et al., 2023; An et al., 2025）、対応するvalue（値）ベクトルが非sinkトークンのものと比較して有意に小さいノルムを示すことを示している（Sun et al., 2024; An et al., 2025）。これは、sinkトークンが異常に大きなattentionスコアでattention出力を支配するのではなく、softmax正規化におけるスケーリング因子として利用していることを示している。この見方は、**GatedAttention（GA）**（Bondarenko et al., 2023; Qiu et al., 2025b; An et al., 2025）の導入によってさらに支持される。明示的なゲーティング機構により、モデルはそのようなリスケーリングを実行でき、attention sinkへの依存を軽減する。もう一つの注目すべき外れ値現象は、残差ストリームにおける**massive activation（MA）**（Sun et al., 2024）である：attention sinkと関連するトークンは、特定の次元で極めて大きな活性化値を示すことが多く、正規化層を通過した後、attention sinkの形成を促進し得る（An et al., 2025）。

上記の両タイプの外れ値は、共通の重要な特性を持つ：正規化を介してその効果を発揮する。我々はこの挙動を**外れ値駆動リスケーリング**という用語で統一する。外れ値が正規化と相互作用して、正規化後の非外れ値成分をリスケーリングするものである。残差内の別種の外れ値（Dettmers et al., 2022; Bondarenko et al., 2023）について、外れ値駆動リスケーリング仮説を検証する。これらの外れ値は、大多数のトークンにわたって固定された次元のセットに現れ、通常値より桁違いに大きな活性化値を示す（図1参照）。実験により、これらの外れ値がattention sinkと多くの性質を共有することが示され、我々はこれを**residual sink**と名付ける。特に、residual sinkは特定の入力に依存せず、データ固有の特徴を伝達していないことを示唆している。さらに、これらがRMSNorm層（Zhang & Sennrich, 2019）と相互作用して外れ値駆動リスケーリングを実行していることを検証する。以下に詳述する広範な実証的証拠がこの仮説を支持する。

**(1)** 正規化の除去（Zhu et al., 2025）はresidual sinkを減少させるが、モデル性能と学習安定性を損なう（3.1節）。クリッピングやアーキテクチャ変更による外れ値の抑制も性能低下をもたらす（3.2節）。

**(2)** 外れ値駆動リスケーリングに依存するモデルにおいて、外れ値が生じやすい次元のRMSNorm重みは平均よりも一貫して大幅に小さい（例：0.006 対 1）ことが観察され、これらの外れ値次元が正規化出力への直接的な寄与者ではなく、主にリスケール因子として機能していることをさらに示唆する。この性質が与えられた場合、*RMSNorm後の特徴ノルムの上限が外れ値の大きさの増加に伴い減少すること*を証明する（付録A.1）。

**(3)** Residual sinkは、attention sinkの学習可能なバイアスへの吸収（Sun et al., 2024; Agarwal et al., 2025）と類似して、パラメータに吸収でき、残差ストリームにおける明示的な外れ値の必要性を排除する（3.3節）。

**(4)** リスケーリングが根本的な目的であれば、明示的に導入することで外れ値が減少すると期待される。attention内のゲーティングベースのリスケーリングがattention sinkを減少させるという先行研究（Bondarenko et al., 2023; An et al., 2025; Qiu et al., 2025b）と一致して、RMSNorm後に軽量なゲーティングを挿入する**GatedNorm**がresidual sinkを効果的に軽減しつつ、モデル性能を維持または向上させることを実験で示す（3.4節）。外れ値が少なく活性化が滑らかなGatedNormで学習したモデルは、より良い量子化性能を示す。

**(5)** ゲーティングベースのリスケーリングは外れ値への依存を減少させ、外れ値を誘発するアーキテクチャ選択への感度を弱める。例えば、SwiGLU（Shazeer, 2020）はFFNにおいてsigmoidベースのGLUを通常上回るが、これはより大きな活性化値を生成して外れ値駆動リスケーリングを支援するためである。ゲーティングベースのリスケーリングを追加すると、GLUはSwiGLUと同等またはそれ以上の性能を達成できる（図3）。

標準softmax attention（Vaswani et al., 2017）、linear attention（Yang et al., 2024）、linear-full attentionハイブリッドアーキテクチャを含む1B、7B、24Bパラメータのモデル、120Bから1Tトークンのデータセットで学習したモデルにわたって仮説を検証する。付録A.2では、外れ値駆動リスケーリングの観点から、attention sinkとresidual sinkのパターン、機能的役割、軽減策を網羅した比較をまとめている。結果は一貫して、softmax attentionとRMSNormにおける正規化での外れ値が病的なアーティファクトではなく、リスケーリング駆動因子であることを示す。residual sinkの機能的役割を理解することで、その機能的利点を維持しつつresidual sinkを減少させる単純で効果的な代替手法を提案し、学習および量子化性能を向上させる。

---

## 2 大規模言語モデルにおける外れ値

本節では、オープンソースLLMにおける外れ値を提示し、それらの相互関係を検討する。pre-norm Transformerに焦点を当てる。長さLの入力系列に対して、モデルはまずトークンを隠れ状態 H₀ ∈ ℝ^{L×d} に埋め込む（dは隠れ次元）。状態はD層を通して処理される。i番目の層関数をFᵢとすると、隠れ状態は以下のように更新される：

$$H_{i+1} = H_i + F_i(H_i), \quad i = 0, 1, \ldots, D-1$$

隠れ状態Hᵢは**残差ストリーム**とも呼ばれ、本研究の主な焦点である。

図1では、異なる外れ値パターンを持つ4つのモデルを分析する：Qwen3-235B-A22B（Yang et al., 2025）、Qwen3-Next（Yang et al., 2025）、GPT-OSS（Agarwal et al., 2025）、DeepSeek-V3（Liu et al., 2024）。各モデルに対して同じ入力系列セットを使用し、全層にわたるattentionマップと残差活性化を記録する。図1では、明瞭さのためattentionマップと隠れ状態を全層で平均化している。隠れ次元dが大きいため、全次元の可視化は実用的でない。外れ値パターンを強調するため、全体的な活性化の大きさで特徴次元を並べ替える。具体的には、各次元jについて、全トークン、全層、全入力にわたる平均絶対活性化値を計算する：

$$H_{\text{avg}}^j = \frac{1}{N(D+1)} \sum_{n=0}^{N-1} \sum_{i=0}^{D} |H_{i,n}^j|$$

ここで $H_{i,n}^j$ はi番目の層、n番目のトークンにおけるj番目の次元の活性化値、Nはトークン数である。次にH_avgの降順で次元をソートする。この並べ替え後、一貫して大きな活性化値を持つ次元が可視化の左側に現れ、attention sinkパターンに酷似した構造を形成する。

**Qwen3-235B-A22B**では、最初のトークンがほぼ全ての他のトークンから一貫して高いattentionスコアを受け取り、attention sinkを示す。対応して、このトークンは2つの次元（例：次元1806と1423）でMAを示す。トークン固有のMAの他に、ほとんどのトークンが次元1423で一貫して大きな活性化値を示すことが観察される。このパターンは入力にわたって安定しており、活性化の可視化では暗い垂直のストライプとして現れ、attention sinkの形状に類似している。後にその正規化時のリスケーリング効果がattention sinkのそれと酷似していることが判明し、これを**residual sink**と名付けることとなった。DeepSeek-V3でも同様のパターンが付録8で観察される：attention sink、MA、residual sinkが全て存在する。

**Qwen3-Next**は、ゲーティングベースのリスケーリングを実行するためにattentionにゲーティングを導入し、attentionロジット外れ値への依存を減少させている。その結果、attention sinkは他のモデルより弱い。さらに、残差ストリームの最大活性化値はわずか38.5で、顕著なMAは観察されない。にもかかわらず、次元1572がトークン全体にわたって他の全次元より一貫して高い活性化値を示し、明確にresidual sinkを示している。

**GPT-OSS**は学習可能なsinkを組み込み、実入力トークンからattention sinkを効果的に除去している。MAも消失する：実トークンの隠れ状態は特定の次元で極端な値を示さなくなる。これは先行研究の解釈と一致する：attention sinkはsoftmax attentionにおける入力非依存のバイアスとして機能する。MAは正規化後にほぼone-hotベクトルを生成してkey空間への射影時に少数の固定行列列のみを活性化させることでこれを可能にする。学習可能なバイアス（例：専用のsink key）が明示的に提供されると、モデルは実トークンからMAを生成する必要がなくなる。しかし、attention sinkとMAが存在しないにもかかわらず、residual sinkはGPT-OSSに残存する。

---

## 3 外れ値駆動リスケーリング

本節では、これらの外れ値の役割に関する詳細な議論と実証的証拠を提供する。pre-norm Transformerで実験を行い、Llama3（Dubey et al., 2024）およびQwen3（Yang et al., 2025）の密モデルの設計に準拠する。アブレーションおよび比較条件が多数あるため、全てのバリアントを一貫した設定で評価する：120Bトークンで学習した2Bパラメータモデル。構造変更がモデルパラメータに影響する場合、総パラメータ数を一定に保つためFFN幅を調整する。完全な実験設定の詳細は付録A.4に記載。分析は5部構成である。

3.1節では、正規化層をDynamic Tanh（DyT）（Zhu et al., 2025; Chen et al., 2025）などの点別関数に置き換えると外れ値が有意に減少することを示す。しかし、DyTは外れ値駆動リスケーリングを提供できないため、学習の安定性と最終性能の両方が低下する。

3.2節では、正規化を保持しても、外れ値を直接制約すること（活性化クリッピングなど）が外れ値駆動リスケーリング機構を破壊し、モデル性能を損ない、時には学習の発散を引き起こすことを実証する。これはまた、sigmoidベースのGLUバリアントなど、外れ値生成を抑制するアーキテクチャ変更が性能低下を示す傾向がある理由を説明する（Shazeer, 2020）。

3.3節では、外れ値を活性化値から学習可能なパラメータにロスなく移転できることを示す：正規化前に軽量な学習可能ベクトルを導入することで、残差ストリームに大きな値を必要とせず、学習可能ベクトル後の増幅された射影を使用して外れ値駆動リスケーリングを引き続き実行できる。

3.4節では、ゲーティングによるリスケーリングの有効化がresidual sinkを性能低下なく減少させることを示す。

3.5節では、ゲーティングベースのリスケーリングが導入され、外れ値への依存が減少すると、アーキテクチャ選択への感度が低下することを発見する。具体的には、DyTが安定した収束を達成し、sigmoidベースのGLUが性能面でSwiGLUと同等またはそれ以上となる。

### 3.1 正規化の除去は外れ値を減少させるが安定性と性能を低下させる

Transformer層の正規化は2箇所にある：attentionのsoftmaxと正規化層（例：RMSNorm）。attentionにおいては、先行研究がsoftmax正規化がattention sinkの主要な原因であることを示している（Gu et al., 2024b; An et al., 2025）。softmaxがsigmoid attention（Gu et al., 2024b）に置き換えられるか、分母が学習可能なバイアスと組み合わされると（Sun et al., 2024; Dong et al., 2024; Agarwal et al., 2025）、attention sinkは消失する。

既存の研究は、attention sinkを示すトークンが他のトークンよりも小さいノルムのvalueベクトルを生成することを発見している（Sun et al., 2024; An et al., 2025）。外れ値駆動リスケーリングの観点からは、attention sinkの存在により、モデルはattention出力におけるほぼゼロの寄与（sinkトークンから）と他の寄与の相対的な配分を調整でき、attention結果のノルムを制御できる。

この解釈は、sigmoid attentionで観察される学習不安定性も説明する：正規化誘起リスケーリングなしでは、初期のattention出力が大きなノルムを持つ（Ramapuram et al., 2024）。

我々の実験では、softmaxベースのattentionをlinear attention（Yang et al., 2024）に置き換えること（トークン混合ステップに正規化なし）もMAを減少させることを発見した。表1の行(3)-(5)に示すように、ピーク活性化値はlinear attentionモデルで510、full attentionとlinear attentionを1:3の比率で組み合わせたハイブリッドモデルで1100に低下し、full attentionベースラインの6000と比較される。注目すべきことに、linear attentionはMAを排除する一方、residual sinkは残存する。

残差の正規化層については、先行研究がRMSNormをDyT（DyT(x) = γ · tanh(αx) + β と定義）に置き換えると外れ値が有意に減少することを示している（He et al., 2024; Owen et al., 2025a）。主な違いはリスケーリングにある：RMSNormでは、各次元が隠れ状態全体の統計量（例：二乗平均平方根）に基づいてスケーリングされ、一つの次元の外れ値が全ての他の次元に影響できる。対照的に、DyTは点別演算のみを使用するため、どの次元も他の次元に直接影響を与えない。

我々の実験はこの制限を確認する。DyTモデルをベースラインと同じ学習率で学習すると、最適化は急速に発散する。これに対処するため、全てのDyT含有設定でハイパーパラメータスイープを実施し、{5 × 10⁻⁴, 1 × 10⁻³, 2 × 10⁻³, 4 × 10⁻³}の学習率を評価し、最良の設定を報告する。元のDyTバリアントは最小学習率5×10⁻⁴でのみ収束し、ピーク活性化値73を示すが、ベースラインと比較してlossが+0.259と有意な性能低下を被る（表1の行(7)対行(1)）。これは、外れ値駆動リスケーリング効果が学習の安定性と最終性能の両方に不可欠であることを示唆する。

これらの結果を総合すると、正規化の除去は外れ値駆動リスケーリング機構を破壊することが示される。その結果、モデルは外れ値を生成しなくなるが、通常は学習の安定性と性能の低下を伴う。

### 3.2 外れ値の直接クリッピングまたは制約は安定性と性能を損なう

次に、正規化を保持しつつ外れ値を抑制する効果を検討する。異なる外れ値の影響を分離するため、2つの設定を考慮する：(1) attention sink、MA、residual sinkを示すモデル、(2) GatedAttention（GA）（Bondarenko et al., 2023; An et al., 2025; Qiu et al., 2025b）によりattention sinkとMAが軽減され、主にresidual sinkを示すモデル。

まず、full attentionベースライン（表1、行(1)）の残差に活性化クリッピングを適用し、閾値を超える活性化値をその値でキャップする。クリッピング閾値が100以下の場合、学習は早期に発散する。閾値1000では、loss曲線に頻繁なスパイクが見られ、より高い最終lossに収束する（+0.006、行(12)）。これは、学習後のモデルでMAやattention sinkをクリッピングすると性能が著しく低下し、しばしばランダムに近い出力を生じるという先行観察と一致する。

次に、attention sinkとMAを減少させるGAと残差クリッピングを組み合わせる。softmax attentionにGAを通じて明示的なリスケーリングが再導入されると、積極的なクリッピング（閾値10）でも収束は可能だが、依然として性能を損なう（+0.003、行(13)）。さらに、同じ設定下で閾値1000でのクリッピングは、はるかに小さい性能低下（+0.001、行(14)）をもたらす。

これらの結果は2つの重要な点を示唆する：(i) attentionにおける外れ値駆動リスケーリング機構が破壊された場合の不安定性の主要な源泉である。(ii) residual sinkも性能に有意に寄与しており、補償なく直接制約すると性能低下を招く。

クリッピング以外に、一見無関係な構成要素である活性化関数を変更することで外れ値を制限するより侵襲性の低い方法を探索し、上記の結論をさらに検証する。先行研究は、Transformerの外れ値が主にFFNに由来することを観察している（Oh et al., 2024; Yona et al., 2025）。現代のほとんどのモデルはGated Linear Unitバリアント、特にSwiGLU（Shazeer, 2020）を採用している：

$$\text{SwiGLU}(x) = x_{\text{down}}(W_{\text{up}}x) \odot \text{swish}(W_{\text{gate}}x)$$

SwiGLUでは、外れ値は $W_{\text{up}}x \odot \text{swish}(W_{\text{gate}}x)$ の項に出現し、$W_{\text{down}}$ によって増幅される。

概念的に、swish活性化をsigmoid（(0,1)の有界範囲を持つ）に置き換えると、FFN出力の大きさが自然に制約され、外れ値の生成が効果的に抑制される。この変更によりSwiGLUは標準的なGLUに帰着する。表1に示すように、GLUは収束時にSwiGLUと比較して有意に小さい外れ値の大きさを示す（1300対6000）が、+0.011の性能低下を被る。これは、sigmoidを使用するGLUバリアントがswishやGELUを使用するものより性能が劣るという先行知見と一致する（Shazeer, 2020）。3.5節では、ゲーティングなどの明示的リスケーリングにより外れ値駆動リスケーリングへのモデルの依存が減少すると、GLUがSwiGLUをわずかに上回ることをさらに観察する。

### 3.3 外れ値のパラメータへの融合

前節では、外れ値が正規化において機能的役割を果たし、補償なく除去すると性能が損なわれることを確立した。理論的には、RMSNorm後の特徴ノルムの上限が外れ値の増大に伴い減少することを証明し（付録A.1）、外れ値が特徴ノルムをリスケーリングすることを可能にする。ここで問う：明示的な外れ値を減少させつつ、この機能性を保持できるか？

先行研究は、学習可能なsinkトークンがattention sinkを入力ロジットから固定パラメータに移行でき（Sun et al., 2024; Dong et al., 2024; Agarwal et al., 2025）、外れ値駆動リスケーリングを保持しつつ実トークンから外れ値を除去できることを示している。これに着想を得て、RMSNorm前に学習可能な要素別スケーリングベクトル（**PreAffine**と呼ぶ）を導入する。具体的には：

$$\text{PreAffineRMSNorm}(x) = \text{RMSNorm}(\lambda_1 \odot x)$$

ここで $\lambda_1 \in \mathbb{R}^d$ は学習可能なベクトルである。residual sinkがほぼ全てのトークンにわたって同じ次元に一貫して現れるため、$\lambda_1$ はそれらの特定の次元を増幅するように学習できる。したがって、入力活性化xに外れ値が含まれていなくても、スケーリングされた入力 $\lambda_1 \odot x$ は選択された次元に大きな値を含むことができ、通常通り外れ値駆動リスケーリングを可能にする。

PreAffineの追加後、ネットワーク内の最大活性化値は2800から640に低下し、最終lossはわずかに改善する（-0.003、表1、行(19)）。24.6B-A1.7Bハイブリッドモデル（設定詳細は第4節）でも各種外れ値削減手法を評価し、対応する活性化統計を図2に報告する。左パネルは、GAがMAを抑制したベースラインを示す。次元304が他の次元より一貫して高い活性化値を示し、residual sinkを示している。中央パネルでは、同じモデルにPreAffineを適用する。いずれか単一の次元での持続的な高活性化値は消失し、ピーク活性化値は1960から988に低下する。

重要なことに、$\lambda_1$ のリスケーリング能力は標準RMSNormパラメータ $\lambda$ のそれとは異なる。$\lambda_1$ はRMSNormと相互作用する：$\lambda_1$ の少数の次元の大きな値がスケーリングされた入力のRMSを制御し、非外れ値次元の強度をリスケーリングする。モデル間で $\lambda$（標準RMSNorm重み）と $\lambda_1$（PreAffine）のパラメータを分析する。完全な結果は付録A.5.1に記載。1からの偏差が最大の次元を特定する。これらのaffineパラメータが最も強い影響を及ぼすためである。観察は以下の通りである：

**(1)** ベースラインモデルでは、$\lambda$ のほとんどの次元は1に近い。しかし、次元304は全層にわたって一貫して1から逸脱し、最小値0.004に達する。特に、ベースラインのresidual sinkも次元304にある。これは、residual sinkによる大きな活性化値が正規化後に直ちにスケールダウンされることを示す。これは、ある次元が外れ値駆動リスケーリングにおける役割を果たした後、その下流への影響が意図的に減衰されることを示唆する。これは、attention sinkトークンのvalueベクトルがより小さいノルムを示すという観察と一致する。

**(2)** PreAffineを備えたモデルでは、$\lambda_1$ の1からの偏差は $\lambda$ のそれよりも有意に大きい。例えば、次元1326の $\lambda_1$ は7.19に達するが、同じ次元の対応する $\lambda$ はわずか0.06である。これは、外れ値が正規化とともに表現を形成するために使用され、その直接的な寄与は抑制されるという見方をさらに支持する。

### 3.4 GatedNorm：明示的なリスケーリングの有効化

PreAffineは残差ストリームの外れ値を減少させるが、正規化計算内（すなわち $\lambda_1 \odot x$ 内）には依然として外れ値が現れる。これに対処するため、GAに着想を得て**GatedNorm**を導入する：全ての正規化層の後に適用される要素別の低ランク自己ゲーティング機構である。形式的に、$y = \text{RMSNorm}(x)$ が与えられた場合、以下を計算する：

$$y_g = \sigma\left(W_{\text{up}}(\text{swish}(W_{\text{down}}(y)))\right), \quad y' = y_g \odot y$$

ここで $W_{\text{down}} \in \mathbb{R}^{d \times r}$、$W_{\text{up}} \in \mathbb{R}^{r \times d}$、$r \ll d$（例：$r = 16$）、$\sigma$ はsigmoid活性化である。

GatedNormは約3.7Mパラメータのみを追加し、2Bモデルの総パラメータの約2%に相当する。パラメータの同等性を維持するため、FFN容量をわずかに削減する。2B密モデルではGatedNormは約5%のレイテンシオーバーヘッドを生じ、このオーバーヘッドはモデルサイズの増加、特にMoEではさらに減少する。より詳細な性能分析は付録A.3に記載。residual sinkを依然として示す表1のいくつかの設定の上にGatedNormを検証する。行(20)-(22)に示すように、GatedNormはGatedAttentionを持つモデル（full attention、hybrid attention、linear attentionを含む）のlossと外れ値の大きさの両方をさらに減少させる。

図2（右）は、24.6B-A1.7BハイブリッドMoEモデルにおいて、GatedNormもresidual sinkを抑制することを示す。GatedNormを使用するモデルの学習されたスケーリングパラメータ $\lambda$ を分析する（付録A.5.1、図7）。$\lambda$ の1からの最大偏差はわずか0.73であり、ベースラインの0.004やPreAffineモデルの0.06と比較される。これは、ネットワークが外れ値駆動リスケーリングに依存しなくなると、正規化後に特定の次元を抑制する必要がなくなることを示す。結果として、正規化出力はより滑らかで量子化に適したものとなる。

異なるゲーティングバリアントも比較し、2つの設計選択に焦点を当てる：ゲーティングの粒度（要素別（スコア形状d）対テンソル別（スコア形状1））と活性化関数（sigmoid、tanh、SiLU、恒等）。知見は以下の通りである。

第一に、sigmoid活性化を伴う要素別ゲーティングがベースラインに対して最も有意な性能改善をもたらす。sigmoidを伴うテンソル別ゲーティングは要素別ゲーティングと同程度にresidual sinkを減少させるが、最終性能はベースラインに近く、要素別バリアントより一貫して劣る。これは、両方の粒度が外れ値を軽減でき（外れ値駆動リスケーリング仮説を支持）、より細粒度の（要素別）リスケーリングがより効果的な調整を可能にし、したがってより良い性能をもたらすことを示唆する。

第二に、要素別ゲーティングを使用する際、sigmoidをtanh、SiLU、または活性化なし（DiTのadaLN（Perez et al., 2018; Xu et al., 2019; Peebles & Xie, 2023; Karras et al., 2024）に類似）に置き換えると、学習中の外れ値ダイナミクスが不安定になる。これは、sigmoidの有界性とゼロ付近の細粒度制御が安定したリスケーリングに有益であることを示し、Chen et al.（2025）と一致する。さらに、テンソル別ゲーティングでは、tanh、SiLU、なしを含む全ての非sigmoid活性化が学習の発散を引き起こし、安定したゲーティングにはsigmoidのような適切に振る舞う有界活性化の必要性をさらに強調する。

### 3.5 GatedNormはアーキテクチャ選択への頑健性を向上させる

3.1節と3.2節は、DyTとsigmoidベースのGLUの両方がベースラインを下回ることを示した。一つの可能な説明は、それらのアーキテクチャが本質的に外れ値駆動リスケーリング機構を制限していることである。本節では、GatedNormを通じて明示的にリスケーリングを提供することで、外れ値への依存を減少させ、それらの性能を回復できるかを調査する。

まず、DyTにGAを装備し（表1の行(8)）、attentionモジュールに明示的なリスケーリングを提供する。これにより、モデルはベースラインの学習率（4.3e-3）で安定して学習できるが、最適な学習率は2e-3である。これは、attentionリスケーリングが学習安定性において重要な役割を果たすことをさらに確認する。次に、GatedNormに類似した低ランク自己ゲーティング機構をDyT層の後に適用し、GatedDyTとする（表1の行(9)）。この追加により、DyTとRMSNormベースラインとの性能差は0.084から0.018に縮小し、attentionだけでなく正規化層自体における明示的なリスケーリングの重要性を強調する。

さらに、GatedNorm導入前後のSwiGLUとGLUを比較する。図3（下部）に示すように、バニラSwiGLUは学習中に4×10⁴を超える最大活性化値を生成するが、バニラGLUは約1×10⁴でピークに達する。外れ値駆動リスケーリングが不足するGLUは、より高いlossに収束する（+0.011）。GatedNormの適用後、GLUは同じ設定下でSwiGLUをわずかに上回る（行(16)で-0.002、行(17)で-0.003）。図3（上部）は、ゲーティングにより、GLUのloss曲線が全バリアント中最悪（青）から最良（緑）に改善することを示す。

これは、GLUとSwiGLUの性能差が主に外れ値駆動リスケーリングを支援する能力の違いに起因することを示唆する。これはまた、ReLU²（Zhang et al., 2024）やPolyNorm（Zhuo et al., 2024）などの外れ値をより容易に生成する高次活性化関数が、そのようなリスケーリングが必要な場合に有利である理由を説明する。GatedNormを通じてリスケーリングが明示的に提供されると、モデルは外れ値生成に影響するアーキテクチャ選択に対して頑健になる。

---

## 4 外れ値軽減のスケーリングとデプロイメントレベルの量子化

本節では、GatedAttentionとGatedNorm、またはPreAffineの異なる組み合わせを大規模設定で評価する。表1の行(5)が示すように、ハイブリッドモデルは利点を示すため、Qwen3-Nextに準拠した効率的なハイブリッドMoEモデルで実験を行う。2つの設定：(1) 活性化パラメータ1.7Bの7.4Bパラメータモデル（MoE-7B-A-2B）、1.2Tトークンで学習、(2) 活性化パラメータ2.7Bの24.6Bパラメータモデル（MoE-24B-A3B）、500Bトークンで学習。この設定では、GatedNormは3%未満のレイテンシオーバーヘッドを生じる。完全な詳細は付録A.4.1に記載。

図4はMoE-24B-A3Bモデルの学習loss曲線と外れ値を示す。観察には以下が含まれる：(1) ベースラインの外れ値は学習初期に急速に上昇し、学習率の低下に伴い徐々に減衰する。PreAffineモデルも初期の外れ値急増（学習の最初の10%以内）を示すが、その後急速に減少する。一つの可能な説明は、学習可能なスケーラー $\lambda_1$ は当初、外れ値駆動リスケーリングを完全に支援するのに十分な大きさを持たず、モデルが一時的に活性化外れ値に依存する必要があること。学習が進み $\lambda_1$ の特定次元が大きくなると、外れ値がパラメータに「吸収」される。GatedNormは外れ値の初期急増を示さず、学習全体にわたって一貫して低い活性化値を維持し、外れ値駆動リスケーリングへの依存の減少を示す。(2) 中期から後期にかけて明確な性能差が現れ、GatedNormがより低い最終lossを達成する。

表2は、BF16および量子化設定における異なる外れ値軽減戦略を比較する。詳細な量子化設定は付録A.4.2に記載。FP8量子化は比較的軽微な変化をもたらすため、積極的なFP4 W4A4量子化に焦点を当てる。結果は、PreAffineとGatedNormの両方のモデルサイズにわたる一貫した改善を示し、GatedNormがより大きな改善をもたらす。特に、GatedNormは知識タスクでベースラインに対して+1.0ポイントの改善を達成し、STEMとコードタスクでは+2.0ポイントを超える。

FP4量子化では以下を観察する：(1) GAとPreAffineのみを使用するモデルに対して、SmoothQuant（Xiao et al., 2023a）は平均+0.5ポイントの改善を提供し、GatedNormに対する改善より大きい。これは、GatedNormのリスケーリングが外れ値感度を大幅に軽減していることを示唆する。(2) GatedNormはFP4量子化後の性能低下が最小（-1.23ポイント）であり、GA（-1.50）やPreAffine（-2.76）と比較される。MGSMベンチマークでは、GatedNormのみが5ポイント以内の性能低下を維持し、他の全手法は約10ポイントの損失を被る。

この優れた量子化耐性は、ゲーティングベースのリスケーリングの外れ値抑制挙動の観察と一致する：|y|が大きい次元はより小さいゲーティングスコア $y_g$ を受け取る傾向があり、より滑らかな最終活性化値をもたらす。全体として、PreAffineは外れ値を再配置するもので、学習可能なsinkやSmoothQuant（Xiao et al., 2023a）と精神的に類似するが、依然として外れ値駆動リスケーリングを実行するために外れ値に依存する。対照的に、GatedNormは明示的なゲーティングを提供し、ネットワーク全体にわたって本質的に滑らかな活性化値を生成し、優れた量子化耐性を達成する。

---

## 5 関連研究

Transformerにおける外れ値は広く研究されてきた。BERTモデルでは、固定次元の外れ値は主にLayerNormの重みとバイアスパラメータに起因し（Bondarenko et al., 2021; Kovaleva et al., 2021; Wei et al., 2022）、特殊トークンのattentionパターンと密接に関連している（Puccetti et al., 2022）。この現象はGPTスタイルのモデルで観察されるattention sinkに類似し、モデル性能に有意な影響を与える（Kovaleva et al., 2021; Xiao et al., 2023b）。BERTで議論される外れ値は、自己回帰モデルにおけるMA（Sun et al., 2024; Gu et al., 2024b; Yu et al., 2024）に大部分が対応する。これらのMAは通常、意味的に疎な特殊トークンに由来し、FFNの初期段階で出現し、残差ストリームを通じて伝搬して、後続層のattention分布に継続的に影響を与える（Sun et al., 2024; Oh et al., 2024; Gu et al., 2024b; Yona et al., 2025）。

いくつかの研究は、GPTモデルの固定次元に一貫して現れる入力非依存の外れ値を特定している（Dettmers et al., 2022）。これらの外れ値はattention sinkとは直接関連しない（He et al., 2024; An et al., 2025）。He et al.（2024）はさらに、これらの外れ値を正規化自体に帰属させ、LayerNorm重みが除去されても残存することを示している。外れ値は学習と推論の量子化の両方を損なうため、多くの手法がその影響の軽減を目指している。一般的な技術には、外れ値による量子化誤差を制限するための行方向、チャネル方向、グループ方向スケーリング（Yao et al., 2022; Xiao et al., 2023a; Wei et al., 2023; Abecassis et al., 2025）、および次元間で外れ値を再分配するためのAdamard変換（Xi et al., 2023; Wang et al., 2025）がある。

他の研究は学習中の外れ値の削減に焦点を当てている。最適化の観点からは、weight decay増加、勾配クリッピング（Ahmadian et al., 2023）、重み分散の制約（Owen et al., 2025b;a; Xie et al., 2026）、または明示的な正則化loss項の追加（Liang et al., 2025）などの戦略が外れ値を抑制できる。一部の研究は、Adamオプティマイザが事前学習における外れ値を引き起こすかどうかも検討している（Kaul et al., 2024; He et al., 2024; Xie et al., 2026）。アーキテクチャの観点からは、先行研究は外れ値と正規化の強い関連を指摘し、外れ値を排除するために正規化の除去を提案している（He et al., 2024; Owen et al., 2025a;b）。我々の研究は、外れ値駆動リスケーリングを明示的に置き換えるアーキテクチャ的介入により、Adam、標準的な学習レシピ、および正規化を使用しながらも、はるかに小さい活性化値でモデルを学習できることを示す。

本研究に最も関連するのは、外れ値の機能的役割を調査する研究である。Bondarenko et al.（2023）; An et al.（2025）は、attention外れ値が文脈認識型スケーリング因子として機能することを提案し、attentionにゲーティングベースのスケーリングを導入することでattention外れ値が減少することを示した。Karras et al.（2020）は、StyleGANの中間特徴マップにおける外れ値の源泉として正規化を特定し、これらの外れ値が正規化時の信号スケーリングに寄与していることを示した。さらに、ゲーティングを通じて生成される入力依存の畳み込み重みが、正規化なしでもこのスケーリング効果を再現できることを示した。このゲーティングベースのスケーリングはadaLN（Perez et al., 2018; Xu et al., 2019; Peebles & Xie, 2023; Karras et al., 2024）でも採用されている。我々の研究はこの知見をLLMのresidual sinkに拡張し、Transformer全体にわたる外れ値駆動リスケーリングの広範な存在を強調し、一連の標的アーキテクチャ介入を通じた体系的な証拠を提供する。

---

## 6 結論

本論文は、LLMにおける外れ値が単なるアーティファクトではなく、機能的役割を持つと主張する。それらは正規化機構（softmaxとRMSNorm）と連携して外れ値駆動リスケーリングを実行し、非外れ値特徴の大きさをリスケーリングする。この機構は安定した学習と高い性能に不可欠である。外れ値を除去し外れ値駆動リスケーリングを破壊するとモデルは損なわれる。ゲーティングベースのリスケーリングを明示的に提供することで、性能を維持または向上させつつ活性化外れ値を減少させることができる。さらに、明示的なリスケーリングの有効化はアーキテクチャ選択への感度を低減する。これらの手法はまた、より滑らかな活性化値をもたらし、特に積極的な低ビット設定下で有意に良好な量子化耐性を実現する。

---

## 限界

本研究は、ネットワーク学習における外れ値駆動リスケーリングの重要性を実証的に示し、モデルがRMSNormなどの正規化を活用して特徴ノルムを調整できることを示した。しかし、なぜこのようなリスケーリングが効果的な学習や表現学習に必要であるかは調査していない。リスケーリングの役割に関するより深い理論的理解は未解決の問題として残る。

---

## 付録

### A.1 外れ値と正規化層の相互作用に関する簡単な計算

以下では、residual sinkがLayerNorm変換後の特徴ノルムをリスケーリングする方法を説明する。入力特徴を $h \in \mathbb{R}^D$ とする。LNのリスケーリングパラメータを $\lambda$ とし、外れ値次元がdの一つだけであると仮定する。

外れ値が非常に小さいaffineパラメータに対応することが観察されるため、$|\lambda_d| \leq \epsilon \|\lambda\|_\infty$ と仮定する。さらに、外れ値が特徴ノルムのr割合に対応すること、すなわち $r = |h_d|/\|h\|_2$ と仮定する。すると以下の不等式が得られる：

$$\|\text{LN}(h)\|_{\text{rms}} \leq \|\lambda\|_\infty \sqrt{(1 - r^2) + \epsilon^2 r^2}$$

これは、LayerNorm後の特徴ノルムの上限が外れ値が大きくなるにつれて減少することを示し、ネットワークが外れ値の大きさを変更することで特徴ノルムをリスケーリングできることを示す。

この不等式を証明するために、以下を定義する：

$$u := \frac{h}{\|h\|_{\text{rms}}}, \quad \|u\|_{\text{rms}} = 1, \quad |u_d| = r, \quad \sum_{i \neq d} u_i^2 = 1 - r^2$$

すると

$$\text{LN}(h) = \lambda \odot \frac{h}{\|h\|_{\text{rms}}}$$

$$\|\text{LN}(h)\|_2 = \frac{\|\lambda \odot h\|_2}{\|h\|_{\text{rms}}} = \sqrt{D} \frac{\|\lambda \odot h\|_2}{\|h\|_2} = \sqrt{D} \|\lambda \odot u\|_2$$

および

$$\|\lambda \odot u\|_2^2 = \lambda_d^2 r^2 + \sum_{i \neq d} \lambda_i^2 u_i^2$$

$\sum_{i \neq d} \lambda_i^2 u_i^2 \leq \|\lambda_{-d}\|_\infty^2 \sum_{i \neq d} u_i^2 = \|\lambda_{-d}\|_\infty^2 (1 - r^2)$ を用いて、以下を得る：

$$\|\text{LN}(h)\|_2 \leq \sqrt{D} \sqrt{\|\lambda_{-d}\|_\infty^2 (1 - r^2) + \lambda_d^2 r^2}$$

### A.2 異なる外れ値の比較

表3は、外れ値駆動リスケーリングの観点からattention sinkとresidual sinkの類似点をまとめている。両現象は正規化層で発生する：attention sinkはsoftmax、residual sinkはRMSNorm。両者は非外れ値成分のスケールを制御するモジュレータとして機能する。manifestationは異なるが（attention sinkはトークン固有、residual sinkは次元固有）、機能的役割は類似している。両者とも下流表現のリスケーリングを可能にする。

重要なことに、両タイプの外れ値はリスケーリング機能を果たした後に積極的に抑制される。これは、attention sinkの小さいvalueベクトルノルムとresidual sinkの小さいRMSNorm affine重みによって証明される。正規化を除去するアーキテクチャ変更はこれらの外れ値を排除するが、性能や安定性を低下させる。逆に、GatedAttentionやGatedNormなどの代替リスケーリング経路を明示的に提供することで、外れ値への依存を効果的に減少させつつ、モデル性能を維持または向上させる。

| 側面 | Attention Sink | Residual Sink |
|---|---|---|
| 発生箇所 | 特殊トークン（例：最初のトークン） | ほとんどのトークン、固定次元 |
| 関連する正規化 | Attentionのsoftmax | RMSNorm |
| 機能的役割 | Attention出力ノルムのリスケーリング | RMSNorm出力ノルムのリスケーリング |
| 除去の効果 | ランダムへの崩壊 | 性能低下 |
| 下流での抑制 | 対応するvalueベクトルが小さい | 対応するRMSNorm affine重みが小さい |
| 削減方法 | sigmoidまたはlinear attentionで減少 | 点別関数で減少 |
| パラメータへの吸収 | 学習可能なsinkトークン/バイアス | PreAffine |
| 明示的リスケーリング代替 | GatedAttention | GatedNorm |

### A.3 GatedNormの効率分析

Megatron-LM（Shoeybi et al., 2019）のZeRO-1オプティマイザ設定を使用して、8層密Transformerでのend-to-end学習オーバーヘッドを評価する。隠れ次元を変化させ、ゲーティング機構の低ランク次元は16に固定する。PreAffineバリアントについては、効率的な実行を確保するためTritonでカスタム融合カーネルを実装する。

| 隠れサイズ | 相対オーバーヘッド |
|---|---|
| 2048 | 8.1% |
| 4096 | 5.9% |
| 8192 | 3.6% |

示される通り、相対オーバーヘッドはモデルスケールの増加に伴い急速に減少する。この傾向は2つの要因で説明される。第一に、GatedNormの計算コストは隠れ次元に線形にスケールし、低ランク次元にはわずかに（定数因子で）依存するだけだが、attentionとFFN層の支配的なGEMM演算は隠れサイズの二乗でスケールする。結果として、GatedNormに起因する総計算の割合はより大きなスケールでますます無視できるものとなる。

第二に、カーネル起動オーバーヘッドは小さい隠れ次元でより顕著である。そのような場合、GatedNormは複数の軽量カーネルから構成され、その起動レイテンシが実行バブルを作り出しハードウェア利用率を低下させる可能性がある。隠れ次元が大きくなると、カーネルあたりのワークロードが起動コストを償却するのに十分に増加し、パイプライン効率を改善して相対オーバーヘッドをさらに削減する。

第三に、MoE設定では、GatedNormの相対オーバーヘッドはさらに小さくなる。MoEモデルは学習ステップ時間を支配する大きな通信およびルーティングコストを伴う。GatedNormは軽量なローカル計算のみを導入するため、総ステップ時間への寄与はこのレジームではさらに希釈され、同規模の密モデルと比較してより低い相対オーバーヘッドとなる。

### A.4 実験設定

#### A.4.1 スケーリング設定

**モデルアーキテクチャ** 密モデルとMoEバリアントの両方を含む多様なアーキテクチャの大規模言語モデルスイートで手法を評価する。アーキテクチャ仕様は表5にまとめている。全モデルはsoftmax attentionにヘッド次元256を使用し、同じ隠れサイズ2048を共有する。2Bモデルは標準的な密Transformerである。対照的に、7B-A2Bと24B-A3Bモデルはlinearとsoftmax attentionをハイブリッド構成で組み合わせたMoEアーキテクチャである：softmax attentionは4層ごとに適用され、残りの層ではlinear attentionが使用される。

| モデル | 2B | 7B-A2B | 24B-A3B |
|---|---|---|---|
| 層数 | 28 | 48 | 24 |
| Softmax Attention間隔 | - | 4 | 4 |
| Softmax Attentionクエリヘッド | 8 | 8 | 16 |
| Softmax Attentionキー/バリューヘッド | 2 | 1 | 2 |
| Softmax Attentionヘッド次元 | 256 | 256 | 256 |
| Linear Attentionヘッド次元 | - | 128 | 128 |
| Linear Attentionバリューヘッド | - | 16 | 32 |
| Linear Attentionクエリ/キーヘッド | - | 8 | 16 |
| 埋め込み共有 | はい | いいえ | いいえ |
| 隠れサイズ | 2048 | 2048 | 2048 |
| FFNサイズ | 6144 | 384 | 512 |
| エキスパート数 | – | 128 | 256 |
| 共有エキスパート数 | – | 1 | 1 |
| Top-k | – | 6 | 8 |

MoEモデルは多数のエキスパート（7B-A2Bで128、24B-A3Bで256）を使用し、top-kルーティング（それぞれk=6、k=8）と1つの共有エキスパートでベースライン容量を確保する。グローバルロードバランスloss（Qiu et al., 2025a）で学習される。埋め込み重みは密2Bモデルでは共有されるが、MoEモデルでは非共有であり、大規模疎アーキテクチャの一般的な慣行に従う。FFN拡張比は大きく異なる：密モデルは広いFFN（6144次元）を使用するが、MoEモデルは各エキスパートでより小さいFFN（384と512）を使用し、エキスパート並列性で補う。Linear attentionヘッドは縮小された次元（128）を使用し、表に詳述される別個のクエリ/キーおよびバリュー射影を持つ。

**評価** 知識、推論、STEM、コード生成、多言語能力にわたる幅広いベンチマークセットでモデル性能を評価する。具体的には、一般知識にMMIU-ReduxとMMIU-Pro、専門レベルの科学的推論にSuperGPQAとGPQA-Diamond、数学的問題解決にGSM8KとMATH、コード生成にCruxEval、MultiPL-E、MBPP、多言語理解にMMMIUとMGSMの結果を報告する。

#### A.4.2 量子化設定

**統一最適化戦略** 活性化外れ値を軽減するため、汎用的な前処理ステップとしてSmoothQuant（Xiao et al., 2023a）を適用する。4096シーケンスのキャリブレーションセットを使用してチャネルごとのスムージング因子のみを計算し、量子化の困難さを活性化から重みに明示的に移行する。

**量子化設定** このスムージングされたベースライン上で、2つのハードウェア整合フォーマットを評価する：

- **FP8（W8A8）**：E4M3フォーマットを使用。重みは128×128のブロックごとスケーリング戦略で量子化、活性化は動的なトークンごと量子化を利用。
- **FP4（W4A4）**：NVIDIA FP4（Abecassis et al., 2025）フォーマットを階層的2段階スケーリングで利用。重みは16のブロックにグループ化され、共有浮動小数点スケール（第1段階）が4ビットマッピング前の範囲を正規化（第2段階）。活性化については、第1段階スケーリングを静的から動的トークンごとに変更して忠実度を維持。

---

## 参考文献

（原論文の参考文献リストを参照）
