# すべてのビットが等しいわけではない：推論モデルのためのスケール依存型メモリ最適化戦略

**Junhyuck Kim<sup>k</sup>, Ethan Ewer<sup>w</sup>\*, Taehong Moon<sup>k</sup>, Jongho Park<sup>b</sup>, Dimitris Papailiopoulos<sup>w,m</sup>**

<sup>k</sup>KRAFTON, <sup>w</sup>ウィスコンシン大学マディソン校, <sup>b</sup>カリフォルニア大学バークレー校, <sup>m</sup>Microsoft Research

*プレプリント. arXiv:2510.10964v1 [cs.LG] 2025年10月13日*

---

## 概要

4ビット量子化は、非推論モデルやゼロショットタスクにおいてスケールを問わずメモリ最適な選択肢として台頭してきたが、本研究では、この普遍的な処方箋が推論モデルには適用できないことを示す。推論モデルでは、モデルサイズよりもKVキャッシュがメモリを支配し得るためである。AIME25およびGPQA-Diamondにおける1,700以上の推論シナリオの体系的な実験を通じて、我々は**スケール依存型のトレードオフ**を発見した：有効サイズが8ビット4Bパラメータ未満のモデルは、長い生成よりも多くの重みにメモリを割り当てた方が高い精度を達成し、一方で大規模モデルはより長い生成にメモリを割り当てた方が高い精度を達成する。このスケール閾値は、並列スケーリングがメモリ効率的になるタイミングや、KVキャッシュの退避が量子化を上回るかどうかも決定する。我々の発見は、LLMのメモリ最適化がスケール非依存であってはならないことを示しつつ、原則に基づいたガイドラインを提供する：小規模な推論モデルではテスト時計算よりもモデル容量を優先し、大規模モデルではテスト時計算を最大化せよ。我々の結果は、推論モデルのデプロイメント最適化には、非推論モデルで確立されたものとは根本的に異なる戦略が必要であることを示唆している。

> **図1：AIME25におけるシリアルテスト時スケーリングのメモリ対精度。** このプロットは、Qwen3ファミリーにおけるpass@1精度と総メモリ（重み＋KVキャッシュ）のトレードオフを示す。モデルの重みはGPTQを用いて4ビットおよび8ビットに量子化されている。各曲線に沿って、バジェットフォーシングにより生成長が増加するにつれてKVキャッシュが成長する。有効サイズが8ビット4B未満のモデルでは、トークンバジェットを飽和まで増加させることはメモリ非効率的である。さらに、数学的推論においては、より高い重み精度（8ビットおよび16ビット）が4ビットよりもメモリ効率的であることが判明した。

*\*この研究はKRAFTONでのインターンシップ中に行われた。*

---

## 1 序論

大規模言語モデル（LLM）の非推論モデルに関する先行のメモリ・性能トレードオフ研究は、主にモデルの重みの圧縮に焦点を当ててきた。これは、モデルの重みが一般にKey-Value（KV）キャッシュよりもはるかに多くのGPUメモリを消費するためである（Dettmers & Zettlemoyer, 2023; Frantar et al., 2022; Lin et al., 2024）。しかし、現代の推論モデルは大幅に多くのトークンを生成し、比例して増加するKVキャッシュが重大なボトルネックとなる。例えば、4ビット重みのQwen3-4Bモデルは2.49 GBを占めるが、32kトークン生成のKVキャッシュには4.42 GB（≈重みの1.8倍）が必要である。このボトルネックはバッチ推論において増幅される：モデルの重みが償却される中、集約されたKVキャッシュが主要なメモリ制約となる。KVキャッシュがメモリの支配的な構成要素となる中、非推論モデルで確立された結果が長い生成を伴う推論タスクでも依然として成立するかは不明である。

本研究では、推論モデルのためのメモリ圧縮の一般原則を調査することを目的とする。従来のモデルサイズと重み精度に加えて、我々の分析は推論モデルのメモリ・精度トレードオフに明確に影響する3つの追加要因を組み込む：生成長、並列スケーリング、およびKVキャッシュ圧縮である。全体として、我々は以下の問いを投げかける：

> **固定メモリバジェットの下で、推論タスクの精度を最大化するために、モデルサイズ、重み精度、テスト時・並列時計算、およびKVキャッシュ圧縮をどのようにバランスさせるべきか？**

我々はQwen3モデルファミリー（0.6Bから32B）（Yang et al., 2025）を対象に、AIME25とGPQA-Diamondの2つのベンチマークで実証的研究を行う。調査は1,700以上の異なるシナリオにわたり、4ビットおよび8ビットのGPTQ重み量子化（Frantar et al., 2022）、2kから30kまでの推論トークンバジェット、最大16サンプルの多数決による並列スケーリング、およびKVキャッシュ圧縮の2つのアプローチ：R-KV（Cai et al., 2025）とStreamingLLM（Xiao et al., 2023b）による退避、およびHQQ（Badri & Shaji, 2023）による量子化を探索する。我々の発見はすべてのタスクやモデルに対する具体的な処方箋を提供するものではないが、精度の損失を最小限に抑えたメモリ効率的な推論モデルのための一般原則を提示する。

### 我々の貢献

**セクション4**では、シリアルテスト時スケーリングの下でモデルの重みとKVキャッシュの間でメモリをどのように割り当てるかを調査する。例えば、KVキャッシュが少ない（すなわちテスト時計算が少ない）32B 8ビットLLMと、KVキャッシュが多い32B 4ビットLLMのどちらがより高い精度をもたらすか？有効サイズ（パラメータ数×重みあたりのビット数）が8ビット4B（≈4.2 GB）未満のモデルでは、モデルの重みにより多くのメモリを割り当てることがより大きなゲインをもたらし、この閾値以上ではパフォーマンスが飽和するまでテスト時バジェットを増加させることにメモリを費やす方が良いことを見出した。

また、重み精度の選択はタスクの性質に依存することを発見した。**知識集約型の推論**では、4ビット重み量子化が広くメモリ最適であり、ゼロショット・非推論モデルにおける4ビット以下の精度の有効性に関する確立された知見（Dettmers & Zettlemoyer, 2023; Frantar et al., 2022; Chee et al., 2023）と一致する。しかし、**数学的推論**では、KVキャッシュが小さい8ビットまたは16ビットのモデル重みの方がしばしば強い性能を提供し、複雑な計算タスクは精度の損失により敏感であることを示唆している。

より長い生成と直交して、生成数を増やすことで大幅なゲインが得られる可能性がある（Brown et al., 2024）が、そのメモリ効率は十分に探索されていない。シリアルスケーリングの上に多数決による並列スケーリングを導入すると、バッチ推論設定を仮定して、グループサイズに比例した大きなKVキャッシュと引き換えにより高い精度というもう一つのトレードオフが生じる。この戦略は、有効サイズが8ビット4B以上のモデルに対してのみシリアルスケーリングよりもメモリ効率的である。興味深いことに、そのようなモデルでは、メモリ最適なグループサイズも総メモリバジェットとともに増加する。

**セクション5**では、KVキャッシュの退避と量子化の両方の手法を考慮して、KVキャッシュ圧縮がメモリ・精度トレードオフにどのように影響するかを調査する。モデルサイズと重み精度を横断して、退避と量子化の両方が、キャッシュ圧縮なしのベースラインを超えてパレートフロンティアを前進させる。圧縮手法の選択は有効サイズによって決定されるべきである：退避は小規模モデル（有効サイズが8ビット4B未満）に対してより良いメモリトレードオフを提供し、一方で両方の戦略は大規模モデルに対して競争力がある。

全体として、推論モデルのメモリ最適戦略は普遍的ではなく、主にモデルの有効サイズによって支配される。我々の主要な実証的発見を以下に要約する：

1. **有効サイズが8ビット4B未満のモデル**では、より長い生成よりもより多くの重みにメモリを割り当てる方がメモリ効率的であるが、大規模モデルはより長い生成から恩恵を受ける。
2. **知識集約型タスク（GPQA-Diamond）**では4ビット重みが広くメモリ最適であるが、**数学的推論タスク（AIME25）**では8ビットまたは16ビット重みの方がメモリ効率的である。
3. **並列スケーリング**は有効サイズが8ビット4Bより大きいモデルに対してのみメモリ・精度トレードオフを改善する。メモリ最適なグループサイズはメモリバジェットとともに増加する。
4. **重み量子化のみ**ではメモリ最適な推論には不十分であり、KVキャッシュの圧縮がより効率的な推論をもたらす。
5. **KVキャッシュの退避**は、有効サイズが8ビット4Bモデル未満の場合、KVキャッシュの量子化よりも良いメモリ・精度トレードオフを提供する。

---

## 2 背景

### 重みのみの量子化

重みのみのポストトレーニング量子化は、再訓練なしにフル精度の重みを低ビット表現に置き換え、メモリ使用量を削減する。重みのみの量子化は、量子化誤差に対してより堅牢であるため、重み・活性化量子化と比較してより低いビット幅が可能である（Yao et al., 2023）。しかし、重みのみの量子化は活性化との乗算前に逆量子化を必要とするため、推論中の計算コストは削減されない。速度の向上はメモリ移動の削減から生じる。本研究では、小さなキャリブレーションセットを使用してレイヤーごとの量子化誤差を最小化し、逆ヘッセ情報を使用して重みを更新する重みのみの量子化手法であるGPTQ（Frantar et al., 2022）を採用する。

### KVキャッシュの量子化

KVキャッシュの量子化は、デコーディング中のメモリフットプリントとメモリ帯域幅を削減するために、キーとバリューのテンソルを低精度で格納する。重みのみの量子化とは異なり、KVの量子化は推論時にオンラインで適用される。プリフィル中に、入力コンテキスト全体のKVテンソルが量子化され、低精度でキャッシュされる。デコード中に、キャッシュされたテンソルはアテンション計算のためにオンザフライで逆量子化される。先行研究では、最新のトークンのための小さなフル精度バッファを慣例的に維持し、デコーディング中に新しいキーとバリューのテンソルをこのバッファに追加する。本研究では、HQQバックエンド（Badri & Shaji, 2023）を用いたキーとバリューの両方のチャネルごとの対称量子化を使用する。HQQはキャリブレーション不要の高速量子化手法であり、オンラインKVキャッシュ量子化に特に適している。

### KVキャッシュの退避

一方で、KVキャッシュの退避もKVキャッシュサイズとアテンション計算のコストを削減する重要な最適化戦略として台頭している。特に推論モデルについては、デコーディング中にKVキャッシュを継続的に退避する動的退避ポリシーを考慮する。StreamingLLM（Xiao et al., 2023b）などの初期の研究は、アテンションシンクとして知られる初期シーケンストークンに加えて、最新のキーとバリューのテンソルを保持するスライディングウィンドウメカニズムを採用している。より最近のR-KV（Cai et al., 2025）は推論モデルのための冗長性認識選択を提案している：デコーディング中にトークンの重要度と冗長性を推定し、非冗長で有益なトークンを共同で選択して保持し、KVキャッシュのごく一部でベースラインに近い精度を報告している。セクション5では、これらの退避ポリシーがKVキャッシュの量子化とともにトレードオフフロンティアをどのように変化させるかを研究する。

### テスト時スケーリング

本研究は、検証器やプロセス報酬モデルなどの外部モデルに依存しないテスト時スケーリング手法に範囲を限定する。推論モデルは通常、拡張された思考連鎖を生成するように訓練され、計画と反省を伴う生成を継続して性能を向上させる（Guo et al., 2025; Jaech et al., 2024; Yang et al., 2025）。これを**シリアルスケーリング**と呼ぶ。Muennighoff et al.（2025）は、モデルの自然な長さを超えてシリアル応答をスケーリングするためのバジェットフォーシングを導入している。モデルが停止しようとすると、指定されたトークンバジェットまでデコーディングを続けるための短いキューが追加される。もう一つの研究ライン、**並列スケーリング**は、複数の独立した推論軌跡を生成する（Brown et al., 2024）。最も単純な形式では、外部モデルなしに、**多数決**が独立にサンプリングされた出力の中で最も頻度の高いものを最終回答として選択する（Wang et al., 2022）。さらなる関連研究はセクション6で議論する。

---

## 3 実験設定

我々は、精度とメモリフットプリントが5つの主要な要因によってどのように影響されるかを測定することで、メモリ・精度トレードオフを体系的に探索する：パラメータ数（*N*）、重み精度（*P<sub>W</sub>*）、テスト時トークンバジェット（*T*）、サンプリンググループサイズ（*G*、*G* > 1は多数決のための複数サンプルを示す）、およびKVキャッシュ圧縮戦略（*π<sub>kv</sub>*、例：退避または量子化）。

メモリコストは以下で与えられる：

> *M* = *M<sub>weights</sub>*(*N*, *P<sub>W</sub>*) + *M<sub>kv</sub>*(*N*, *π<sub>kv</sub>*, *T*, *G*)

ここで、*M<sub>weights</sub>*は重みのメモリフットプリントであり、おおよそ*N* · *P<sub>W</sub>*に比例する。なお、本論文を通じて、**モデルサイズ**はパラメータ数*N*を指し、**有効サイズ**または**スケール**は重みのメモリフットプリント*M<sub>weights</sub>*を指す。*M<sub>kv</sub>*はKVキャッシュメモリであり、*π<sub>kv</sub>* = 退避の場合を除き、おおよそ*N*、*G*、*T*に比例する。退避の場合、コストは一定のトークンバジェットを超えると一定になる。正確なメモリコスト方程式については付録A、モデル固有の値については表1を参照されたい。

### 表1：評価モデルのメモリフットプリント

| モデル | モデル重み (GB) | | | KVキャッシュ (GB) | | | |
|---|---|---|---|---|---|---|---|
| | 4ビット | 8ビット | 16ビット | 2kトークン | 18kトークン | 30kトークン | 30kトークン × 16サンプル |
| Qwen3-0.6B | 0.50 | 0.71 | 1.40 | 0.21 | 1.92 | 3.20 | 51.27 |
| Qwen3-1.7B | 1.26 | 1.93 | 3.78 | 0.21 | 1.92 | 3.20 | 51.27 |
| Qwen3-4B | 2.49 | 4.19 | 7.49 | 0.27 | 2.47 | 4.12 | 65.91 |
| Qwen3-8B | 5.68 | 8.94 | 15.26 | 0.27 | 2.47 | 4.12 | 65.91 |
| Qwen3-14B | 9.30 | 15.50 | 27.51 | 0.31 | 2.75 | 4.58 | 73.24 |
| Qwen3-32B | 18.01 | 32.66 | 61.02 | 0.49 | 4.39 | 7.32 | 117.19 |

### モデル

我々はQwen3モデルファミリー（Yang et al., 2025）で実験を行う。このファミリーは0.6Bから32Bパラメータの範囲であり、幅広いモデルサイズを提供するため、スケール横断の細粒度な体系的研究に適している。

### タスク

実験は、相補的な難易度プロファイルを持つ挑戦的なベンチマークで実施される。**AIME25**（AIME, 2025）は、多段階推論を要求するコンテスト級の数学ベンチマークである。対照的に、**GPQA-Diamond**（Rein et al., 2024）は、化学、生物学、物理学などの分野にまたがる科学的知識と統合的推論を重視する（Li et al., 2025）。

### 推論の詳細

特に指定のない限り、温度0.6でサンプリングし、インスタンスあたり32回の生成の平均精度を報告する。Muennighoff et al.（2025）に従い、バジェットフォーシングによるシリアルスケーリングでは、生成が目標トークンバジェットより早く終了した場合、文末トークンをプロンプト「Wait」で置換し、目標バジェットに達するまでデコーディングを続ける。目標バジェットに達すると、プロンプト「**Final Answer**\n\\boxed{」を注入する。2kから30kまで4k刻みでトークンバジェットを評価する。コードは https://github.com/krafton-ai/not-all-bits-are-equal で利用可能である。

---

## 4 重みのみの量子化を用いたテスト時スケーリング

限られたメモリの下で最高の性能を目指す場合、モデルの重みとKVキャッシュの間でメモリをどのように割り当てるべきか？さらに、モデルの重みにスペースを割り当てる際、低精度のより多くのパラメータと高精度のより少ないパラメータのどちらが良いか？

これらの問いに答えるために、テスト時トークンバジェット（*T*）を変化させながら、異なるモデルサイズ（*N*）と重み精度（*P<sub>W</sub>* ∈ {4, 8, 16}）でテスト時スケーリングを研究する。モデルを4ビットおよび8ビット精度に量子化するためにGPTQを使用する。この分析では、*π<sub>kv</sub>*をすべてのキャッシュエントリを保持する設定（退避なし、フル精度）に固定し、まずサンプリンググループサイズ*G* = 1の結果を提示する。*G* > 1の並列スケーリングおよび他の*π<sub>kv</sub>*ポリシーについては後で議論する。

> **図2：パレート最適構成の組成（AIME25、Qwen3）。** トークンバジェット(a)と有効モデルサイズ(b)が、図1のパレートフロンティア上の構成の総メモリバジェットに対してプロットされている。プロットは戦略的シフトを示す：低メモリバジェット（<10 GB）では有効モデルサイズの増加がメモリ効率的であるが、高バジェットではトークンバジェットの増加が性能改善の支配的な戦略となる。

図1は、フル精度KVキャッシュを用いたシリアルスケーリングの下での精度対総メモリのパレートフロンティアを明らかにする。このフロンティア上の構成を分析することで、固定メモリ制約内でのモデル選択、重み精度、テスト時バジェットの最適化に関する実用的な推奨事項を提供する。

### 有効サイズが8ビット4B未満のモデルでは、テスト時バジェットを飽和まで増加させるよりも、有効モデルサイズの増加にメモリを費やす方が良い

小規模モデルの生成バジェットを延長することは、大規模モデルを使用するのと比較して、より高いレイテンシと引き換えにより低いメモリ使用量を得る方法としてしばしば見なされるが、我々の分析は、これが見かけ倒しの経済であることを明らかにする。実際、有効サイズが8ビット4B未満のモデルでは、この戦略は総メモリにおいてしばしば最適でない。図2は、8 GB未満のメモリバジェットでは、パレートフロンティアがトークンバジェットではなくモデルサイズの増加によって主に前進することを示している。例えば、8ビットの1.7Bモデルと6kトークンバジェットは、8ビットの0.6Bモデルと18kトークンバジェットを上回る。同様に、4ビットの4Bモデルと10kトークンバジェットは、8ビットの1.7Bモデルと18kトークンバジェットを凌駕し、同様のメモリバジェットの下でより大きな有効サイズのモデルを選択する方が良いことを実証している。レイテンシ分析（セクション4.1）が確認するように、より大きな有効サイズの構成はエンドツーエンドのレイテンシがトークンバジェットに支配されるため、より高速でもあり、有効モデルサイズの増加が厳密に優位な選択となる。

### 有効サイズが8ビット4B以上の大規模モデルでは、性能が飽和するまでテスト時バジェットを増加させることにメモリを使用する方が効率的

小規模モデルの戦略とは直接対照的に、大規模モデルでは生成バジェットの延長が精度を改善するためのよりメモリ効率的な方法である。この戦略的シフトは図2に明確に示されており、10 GBより大きいメモリバジェットでは、パレートフロンティア上の最高性能構成が一貫して20k以上のトークンバジェットを特徴としている。この領域では、トークンバジェットの増加が精度改善の支配的な方法となる。

> ### 発見1
> モデルの重みとKVキャッシュの間のメモリ効率的な割り当て戦略はスケール依存的である。有効サイズが8ビット4B未満のモデルでは、有効モデルサイズの増加にメモリを割り当てる方が効率的である。この閾値以上のモデルでは、性能が飽和するまでテスト時バジェットを増加させる方がメモリ効率的となる。

我々の分析は主に各推論インスタンスがモデル全体とKVキャッシュを使用するシナリオを仮定しているが、実際にはモデルの重みは複数の同時生成にわたって償却可能であり、メモリダイナミクスを根本的に変化させる。図3は、モデルの重みが複数の同時生成にわたって共有される場合のメモリ・精度トレードオフの変化を検討している。理論的バッチサイズが増加するにつれて、重みコストがより多くの生成にわたって償却されるため、小さなモデル重みの利点は減少する。理論的バッチサイズ16では、0.6Bモデルはパレートフロンティアに一度も現れないことがわかった。4ビットおよび8ビット重み精度の8Bおよび14Bモデルと、8ビットおよび16ビット精度の4Bモデルは、理論的バッチサイズ16の場合、1〜4 GBの生成あたりメモリ領域で有利なトレードオフを示す。特に、8ビットの4Bモデルは1〜2 GB領域で一貫してパレートフロンティア上に位置する。

> **図3：異なる理論的バッチサイズでのメモリ対精度（AIME25、Qwen3）。** 各サブプロットは、異なる理論的バッチサイズでの生成あたりメモリ対精度を示す。モデル重みメモリは同時生成にわたって償却される。バッチサイズの増加に伴いパレートフロンティアがシフトし、モデル重みの償却が最適なメモリ割り当て戦略にどのように影響するかを明らかにする。

### メモリ最適な重み精度はタスクおよびサイズに依存する

単一バッチ推論設定（図1および図2）のトレードオフに戻ると、我々の発見は、数学的推論タスクでは4ビット重み量子化が一貫してメモリ非効率的であることを示す。AIME25ベンチマークでは、小規模モデル（*N* ∈ {0.6B, 1.7B}）に対して8ビットがメモリ最適である。これは、4ビット量子化によって節約されたメモリをより大きなトークンバジェットに再割り当てしても、精度損失を補償するには不十分であるためである。この4ビットの非効率性はより大きな*N*でも持続し、8ビットおよび16ビットの構成が同等のメモリで高い精度を達成する。これは図2(b)に示されており、6 GBを超えるメモリバジェットでは、8ビットまたは16ビットの重みがフロンティア上で最もメモリ最適であることが多い。特に、8ビットの8Bモデルは4ビットの14Bモデルを一貫して上回り（図1）、4ビットの32Bモデルは8ビットの14Bモデルと16ビットの8Bモデルの両方に厳密に支配される。このような発見はDettmers & Zettlemoyer（2023）とは直接対照的である。しかし、知識集約型タスクでは、4ビット量子化が広くメモリ最適であることがわかった。GPQA-Diamondの図4に示されるように、フロンティアは低精度に有利にシフトする。これは、異なるタスクタイプがモデルパラメータに異なる要求を課すことを示唆する。数学的推論は重み内の数値精度に依存する可能性があり、これは積極的な4ビット量子化によって損傷される。一方、知識集約型タスクは知識容量を増やすためにパラメータ数の最大化を優先し、大規模な4ビットモデルをよりメモリ効率的にする。

> ### 発見2
> 知識集約型タスクでは、4ビットが広くメモリ最適である。数学的推論タスクでは、より高い精度が必要である。小規模モデル（*N* ∈ {0.6B, 1.7B}）では8ビットがメモリ最適であり、より大きなパラメータ数では8ビットと16ビットの両方が競争力を持つ。

> **図4：GPQA-Diamondでのメモリ対精度（Qwen3）。** GPQA-Diamondにおけるシリアルスケーリングのメモリ・精度トレードオフ。総メモリはモデルの重みとKVキャッシュの合計。各曲線に沿った点はトークンバジェットの増加を表す。知識集約型タスクでは4ビット重みが広くメモリ最適である。

トークンバジェットの増加によるシリアルスケーリングに加えて、サンプリンググループサイズ（*G*）の増加による並列スケーリング軸を導入できる。バッチ推論設定を仮定すると、KVキャッシュは*G*とともに成長し、より高い精度と引き換えになる。これはもう一つの重要な問いを提起する：

> **より大きな有効モデルサイズやより長い生成長と比較して、並列サンプルにメモリを割り当てる方がメモリ効率的なのはいつか？**

### 並列スケーリングの有効性はスケール依存的である

体系的な評価のために、バジェットフォーシングを使用して*G*個の並列サンプルそれぞれのトークンバジェットを制御し、多数決を使用して最終回答を選択する。図5は、並列スケーリングがメモリ・精度トレードオフにどのように影響するかを示す。点線はシリアルスケーリングのみからのパレートフロンティアを示す。各色の曲線は、グループサイズ*G*を増加させた場合の特定のモデル構成のフロンティアを表す（モデルごとの内訳については付録B、図10を参照）。有効サイズが8ビット4B未満のモデルでは、並列スケーリングはメモリ非効率的であり、その構成はシリアルスケーリングのみによって確立されたフロンティアの下に位置する。しかし、大規模モデルでは並列スケーリングがトレードオフを改善し、グローバルパレートフロンティア上のメモリ最適グループサイズ*G*はメモリバジェットとともに増加する。4 ≤ *G* < 8のグループサイズは16.4〜28.9 GBの範囲でメモリ最適であり、28.9 GBを超えるバジェットではさらに大きなグループ（*G* ≥ 8）によってフロンティアが押し上げられる。

> ### 発見3
> 有効サイズが8ビット4B未満のモデルでは、シリアルスケーリングのみが並列スケーリングよりも良いメモリ・精度トレードオフを提供する。この閾値以上の有効サイズのモデルでは、並列スケーリングがトレードオフを改善し、グローバルパレートフロンティア上のメモリ最適グループサイズ*G*はメモリバジェットとともに増加する。

---

## 4.1 レイテンシとスループットの分析

我々は主にメモリ・精度トレードオフに焦点を当てているが、レイテンシとスループットも重要な実用的考慮事項である。このセクションでは、モデルサイズ、重み精度、生成長が両方の指標にどのように影響するかを分析する。

### 実験設定

すべての測定は、vLLMフレームワーク（Kwon et al., 2023）とアテンションバックエンドとしてFlashAttention（Dao, 2023）を使用して、単一のNVIDIA A100 80 GB GPU上で実行される。特定のトークンバジェットに対するスループットを測定するために、バッチサイズの範囲をスイープし、アウトオブメモリエラーやKVキャッシュのプリエンプションなしに正常に完了する最大バッチサイズを記録する。

> **図6：レイテンシ対精度のトレードオフ（AIME25、Qwen3）。** 各曲線は、生成長の増加に伴う異なるモデルサイズと重み精度でのエンドツーエンドレイテンシ対精度を示す。生成長がレイテンシを決定する支配的な要因として浮上し、重み量子化は大規模モデル（14B、32B）でより顕著な速度向上を提供する。

> **図7：スループット対精度のトレードオフ（AIME25、Qwen3）。** 各点は、生成長の増加に伴う80 GB VRAM制約下での最大スループット（リクエスト/秒）対精度を表す。小規模モデルはより高いバッチサイズを達成できるが、フロンティアはモデル能力と生成効率のバランスを取る構成によって支配される。

図6において、生成長がすべてのモデル構成にわたるエンドツーエンドレイテンシを決定する支配的な要因であることを示す。メモリ移動コストの削減による重み量子化のレイテンシへの恩恵は、小規模モデル（8Bまで）では控えめであるが、大規模モデル（14B、32B）では顕著になる。例えば、14Bモデルは16ビット精度で6kトークンの生成に137.7秒かかるが、4ビットバリアントは10kトークンを130.1秒で生成する。図7に示されるスループットの全体的な傾向も同様である。

レイテンシとスループットの両方において、8ビットおよび16ビット精度の4Bモデルが最も強い速度・精度トレードオフを一貫して示す。重要なことに、4ビット精度はいかなるモデルサイズでもパレートフロンティア上に存在せず、強化学習のロールアウトなどの速度重視のアプリケーションでは、8ビットなどのより高い重み精度が最適な選択肢である可能性を示唆する（Liu et al., 2025a）。トレードオフはスケールの極端でより不利になる：小規模モデル（0.6B、1.7B）は最大160および170の極端なバッチサイズを達成し、2kトークン生成で2.9および2.64リクエスト/秒のスループットを提供するが、精度は根本的に制限される。逆に、32Bモデルは遅い生成速度と大きなメモリフットプリントのためスループットが低く、80 GB VRAMの制約下でバッチングが制限される。

> ### 発見4
> レイテンシとスループットの両方において、4ビット精度はパレートフロンティア上に存在しない。より高い精度（8ビットおよび16ビット）が精度と速度のより良いトレードオフを一貫して提供する。

---

## 5 重みおよびKVキャッシュ圧縮を用いたテスト時スケーリング

これまでの分析は、より多くのトークンを割り当てることが一般に精度を向上させるが、特にKVキャッシュが総メモリを支配し得る有効に小さなモデルでは、常にメモリ効率的であるとは限らないことを示している。量子化や退避によるKVキャッシュの圧縮はこのフットプリントを削減できるが、精度コストを伴う可能性がある。これは以下の問いを提起する：

> **KVキャッシュ圧縮戦略（退避および量子化）は全体的なメモリ・精度トレードオフをどのように変化させ、どちらのアプローチがより強力な推論につながるか？**

これに答えるために、モデルサイズと重み精度にわたって両方の圧縮戦略を評価する。退避については、ターゲットKVバジェット2k、4k、8kトークンのR-KVを使用する。KVキャッシュ量子化については、グループサイズ64、フル精度残差バッファ128トークンで、2ビット、4ビット、8ビット精度への対称チャネルごと量子化を使用する。結果はインスタンスあたり8回の生成の平均である。まず両方の手法が広く有益であることを示し、次にどの戦略がどの条件で最適かの詳細な分析を提供する。

> **図8：KVキャッシュ圧縮戦略別のメモリ対精度（AIME25、Qwen3）。** プロットは、バジェットフォーシングを用いたシリアルスケーリングの下で、モデルサイズと重み精度にわたるKVキャッシュ圧縮のパレートフロンティアを示す。退避はトークンバジェット2k、4k、8kのR-KVを使用。量子化は対称チャネルごと（グループサイズ64）で2ビット、4ビット、8ビット。薄い背景線は個別の（モデルサイズ、重み精度、KV戦略）構成の曲線を示す。両方の圧縮戦略がメモリ・精度トレードオフを一貫して改善する。

### KVキャッシュの退避と量子化は、テストされたすべてのモデルサイズと重み精度にわたってパレートフロンティアを一貫して前進させる

図8に示される最初の重要な発見は、量子化と退避の両方の集約パレートフロンティアが、4ビット、8ビット、16ビット重みのモデルに対して、圧縮なしのベースラインを決定的に超えて前進することである。この改善は、これらの戦略が同じメモリバジェットでより高い精度、または同じ精度でより低いメモリコストを実現することを実証する。恩恵は10 GB未満の低メモリ領域で特に顕著であり、小規模モデルがKVキャッシュによって最も制約される。これは、モデルの重みが積極的に圧縮されている場合でも、KVキャッシュには活用可能な重大な冗長性が含まれていることを示す。したがって、我々の結果はKVキャッシュ圧縮を推論モデルのメモリ効率的なデプロイメントに不可欠かつ広く有益な戦略として確立する。

> ### 発見5
> 重み量子化のみではメモリ最適な推論には不十分である。KVキャッシュ圧縮はすべての重み精度にわたってメモリ・精度フロンティアを前進させる。

KVキャッシュ圧縮が広く有益であることを確立した上で、次に所与のモデルサイズ*N*と重み精度*P<sub>W</sub>*に対して、量子化と退避のどちらの圧縮戦略が好ましいかを分析する。図9は結果として得られるメモリ・精度トレードオフを示し、各戦略が曲線を異なる形状にする。量子化はトークンあたりのメモリコストを削減し、曲線を左方向にシフトさせるが、通常は精度の低下を伴う。対照的に、退避はKVキャッシュに固定のメモリ上限を強制し、メモリ使用量が一定のまま精度が向上する特徴的な垂直曲線をもたらす。

> **図9：KVキャッシュ戦略別のモデルごとのメモリ対精度（AIME25）。** 各プロットは、単一のモデルサイズと重み精度に対するメモリ・精度トレードオフを示し、フルKVキャッシュベースラインとR-KV退避および対称チャネルごと量子化を比較する。各曲線に沿った点は、バジェットフォーシングによる処理トークン数の増加を表す。

### 小規模モデルでは退避が量子化よりも効果的

有効サイズが8ビット8Bモデル未満のモデルでは、退避が一貫して最良のメモリ・精度トレードオフを提供する。図9のフル精度4Bモデルに示されるように、8kトークンバジェットの退避は、総メモリを大幅に削減しながら最大精度をほぼ損失なく維持する。この観察はすべての重み精度にわたって4Bモデルで成り立つ（これらの結果については付録C、図12を参照）。対照的に、積極的な4ビットKVキャッシュ量子化は、これらの小さな有効サイズで精度の大幅な低下を引き起こす。これは、有効に小さなモデルが量子化によって導入される数値誤差により敏感であるのに対し、退避はより小さく重要なトークン群のフル精度を保持することを示唆する。例えば、4ビット重み精度の1.7Bモデルでは、退避が高精度を維持しながら最良のメモリトレードオフを達成するが、8ビット量子化KVキャッシュは効果的であるものの、同等の性能レベルに到達するために大幅に多くのメモリを必要とする。

### 大規模モデルでは量子化が退避と競争力を持つ

有効サイズが8ビット8Bモデルより大きいモデルでは、退避の明確な優位性が薄れ、量子化が非常に競争力のある戦略となる。16ビット重みの8Bモデルでは、量子化と退避が同等のメモリ・精度トレードオフを達成する。4ビットKVキャッシュ量子化は競争力があるが、より小さなバジェット（2kまたは4k）の退避は低メモリ領域で同様のトレードオフを提供する。これは、有効パラメータ数が多い大規模モデルが、量子化による精度損失に対してより堅牢であることを示唆する。しかし、より積極的な2ビット量子化は依然として精度の大幅な損失をもたらすことがわかった。

> ### 発見6
> KVキャッシュ退避は、有効サイズが8ビット8Bモデル未満のモデルに対して、KVキャッシュ量子化よりも良いメモリ・精度トレードオフを提供する。この閾値以上のモデルでは、量子化がますます競争力のある戦略となる。

---

## 6 関連研究

### 訓練時スケーリングと知識容量

基礎的なスケーリング研究（Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022）は、モデルサイズ、データ、損失の間のべき乗則関係を確立し、固定計算バジェットの下での計算最適な訓練のための処方箋を提供する。これらの結果は事前訓練中のパラメータとトークンの割り当てに関する指針を提供するが、推論時計算を考慮しておらず、新たな外挿が必要である（Gadre et al., 2025）。並行して、容量指向の分析は、パラメータあたりの情報として知識をモデル化するか、記憶と汎化を測定することで、モデルが格納できるものを推定する（Morris et al., 2025; Allen-Zhu & Li, 2024）。これらの見方はバジェット中心の視点を動機付けるが、デプロイメント制約下での精度と推論時トレードオフは未指定のままである。ビット正規化研究は、異なる精度での性能が総モデルビット（Dettmers & Zettlemoyer, 2023）や訓練データ量（Kumar et al., 2024）とどのようにスケーリングするかを調べ、特にゼロショットやフューショットのシナリオで行われている。Feng et al.（2025）; Mekala et al.（2025）は、数値精度の低下が算術推論や長コンテキスト性能を顕著に損なう可能性があることをさらに示し、より大きなモデルサイズによる補償がなければ、精度、タスク構造、コンテキスト長の間の相互作用を示唆する。

### 推論時手法とスケーリング則

Chain-of-thoughtプロンプティングは中間ステップを引き出し、Self-consistencyは多様な根拠をサンプリングし多数決により集約することで性能を向上させる（Wei et al., 2022; Wang et al., 2022）。現代の推論モデルは大幅に多くのトークンを生成するよう訓練され、ベンチマーク全体で大きなゲインをもたらす（Wang et al., 2022; Wei et al., 2022; Brown et al., 2024; Muennighoff et al., 2025; Guo et al., 2025; Yang et al., 2025; Comanici et al., 2025; Jaech et al., 2024; Qwen Team, 2025; Kimi Team, 2025）。テスト時スケーリング則は、FLOPs、トークン、または生成数の増加に伴い性能がどのように変化するかを研究し、多数決、best-of-n、検証ベースの探索などの戦略を比較する（Brown et al., 2024; Wu et al., 2024; Snell et al., 2024; Muennighoff et al., 2025; Sadhukhan et al., 2025; Wang et al., 2025; Zhao et al., 2025）。しかし、これらの研究は、メモリとレイテンシを削減するがFLOPsに影響しない重みのみの量子化などの圧縮技術の影響を捉えていない。Liu et al.（2025b）とKurtić et al.（2025）が推論モデルにおける量子化を同時期に研究しているが、彼らの焦点はメモリトレードオフではなく精度の劣化にある。我々の研究はメモリ中心の視点で独自性を持つ：固定メモリバジェットをモデルの重みとテスト時計算（生成長と並列性）の間で割り当てるトレードオフを分析し、KVキャッシュの全コストを組み込んでいる。また、KVキャッシュの退避を含むように圧縮技術の範囲を拡大している。

### 効率的推論

LLM量子化の課題、特に外れ値の処理に対処するためのさまざまな戦略が提案されている（Frantar et al., 2022; Xiao et al., 2023a; Lin et al., 2024; Kim et al., 2023; Dettmers et al., 2022）。量子化認識訓練は、順伝播で量子化された重みを用いてモデルを訓練することでこのアイデアを拡張する（Liu et al., 2023a; Ma et al., 2024; Liu et al., 2025c）。ポストトレーニングKVキャッシュ圧縮技術は退避と量子化に分類できる。退避手法は異なる基準に基づいて重要度の低いエントリを選択的に破棄し（Cai et al., 2025; Xiao et al., 2023b; Zhang et al., 2023; Liu et al., 2023b; Li et al., 2024; Ge et al., 2023）、量子化アプローチはキャッシュされた値の精度を削減する（Badri & Shaji, 2023; Kang et al., 2024; Liu et al., 2024; Kim et al., 2024; Hooper et al., 2024）。

---

## 7 結論

固定メモリバジェットを伴う実世界の状況下で、推論モデルのデプロイメントは究極的にはバイトをどこに使うかの問題であり、実務者は無数の選択肢を提示される。我々の研究は、この制約を中心にテスト時スケーリングを再定式化する。我々は、推論モデルのモデルサイズ、重み精度、KVキャッシュ圧縮、トークンバジェット、サンプリンググループサイズにわたるメモリ割り当てのトレードオフを研究する。推論モデルのメモリ最適推論戦略は画一的な処方箋ではあり得ず、モデルの容量（有効サイズによって決定）とタスクの性質に依存することを見出した。

**小規模モデルサイズ**（一般に8B未満のモデル）では、モデルの重みを優先することがより良いメモリ・精度トレードオフをもたらし、数学的推論にはより高精度の8ビットまたは16ビット重みを使用し、量子化よりもKVキャッシュの退避を選好する。**大規模モデル**では、性能が飽和するまでトークンバジェットを増加させ、並列スケーリングを活用することが支配的な戦略となる。重要なことに、追加のKVキャッシュが追加のモデル重みを上回る変曲点は、モデルがより洗練されるにつれて変化する可能性がある。しかし、FLOPsベースのテスト時スケーリング則から実用的なメモリ制約に焦点をシフトすることで、我々のフレームワークと分析は推論モデルを効果的にデプロイするための一般原則を提供する。

---

## 8 制限事項と今後の研究

我々の範囲は、探索空間を扱いやすくし推論のみに保つために意図的に限定されている。テスト時スケーリングについては、シリアルスケーリングにプロンプト注入を、並列スケーリングに多数決を使用し、外部モデルを必要とする手法を意図的に除外している。我々の分析には、量子化認識訓練を含む、異なるポストトレーニング量子化やKVキャッシュ退避アルゴリズムの比較研究も含まれていない。Qwen3ファミリーで評価を行い、その広いサイズ範囲と固定アーキテクチャを理由に選択し、2つの挑戦的なベンチマーク（数学的推論のためのAIME25と知識集約型推論のためのGPQA-Diamond）を使用した。これらの選択は、すでに1,700以上の実験構成にわたる扱いやすい探索空間を維持し、自己完結型の推論戦略に焦点を当てるために必要であり、手法のより広範な比較は今後の研究の明確な方向性として残されている。

---

## 付録A メモリ方程式

総メモリコスト*M*は、モデルの重みに必要なメモリ*M<sub>weights</sub>*とKVキャッシュ*M<sub>kv</sub>*の合計である。

### 重みメモリ

重みの総メモリフットプリントは、量子化パラメータと非量子化パラメータのメモリの合計である。一般的な方程式は：

> *M<sub>weights</sub>* ≈ [*N<sub>quant</sub>* · (*P<sub>W</sub>*/8) + (*N<sub>quant</sub>*/*g<sub>W</sub>*) · ((*P<sub>S</sub>* + *P<sub>Z</sub>*)/8)] + [*N<sub>unquant</sub>* · (*P<sub>native</sub>*/8)] [バイト]

ここで、*N<sub>quant</sub>*と*N<sub>unquant</sub>*はそれぞれ量子化パラメータと非量子化パラメータの数、*P<sub>W</sub>*は重みの低ビット精度、*g<sub>W</sub>*はグループサイズ、*P<sub>S</sub>*と*P<sub>Z</sub>*はスケールとゼロポイントのビット幅、*P<sub>native</sub>*は非量子化レイヤーのネイティブ精度である。

GPTQを使用する我々の具体的な設定では、大きな線形レイヤーが量子化され、トークン埋め込み行列、正規化レイヤーの重み、最終言語モデルヘッドなどのコンポーネントはネイティブBF16精度のまま残される。我々の実験では、グループサイズ*g<sub>W</sub>* = 128、スケール精度*P<sub>S</sub>* = 16（FP16）、対称量子化によりゼロポイント精度*P<sub>Z</sub>* = 0を使用する。

### KVキャッシュメモリ

圧縮なしの場合、KVキャッシュメモリは以下で与えられる：

> *M<sub>kv</sub>* = *G* · *T* · *n<sub>layers</sub>* · *n<sub>kv_heads</sub>* · *d<sub>head</sub>* · 2 · (*P<sub>native</sub>*/8) [バイト]

ここで、*G*はサンプリンググループサイズ、*T*はトークン数、*n<sub>layers</sub>*はレイヤー数、*n<sub>kv_heads</sub>*はKey/Valueヘッド数、*d<sub>head</sub>*はヘッドあたりの次元、係数2はKeyとValueの両テンソルを説明し、*P<sub>native</sub>*はキャッシュ要素のネイティブ精度（ビット単位、例：BF16では16）である。

メモリコストは異なるKVキャッシュ戦略によって修正される：

- **退避：** この戦略は格納されるトークン数を削減する。メモリコストは以下の通り：
  > *M<sub>kv</sub>* = *G* · min(*T*, *T<sub>retain</sub>*) · *n<sub>layers</sub>* · *n<sub>kv_heads</sub>* · *d<sub>head</sub>* · 2 · (*P<sub>native</sub>*/8)
  
  ここで*T<sub>retain</sub>*はポリシーによって保持される最大トークン数。我々の実験ではR-KVを使用し、*T<sub>retain</sub>* ∈ {8192, 4096, 2048}をテストする。

- **量子化：** この戦略は精度を削減するが、量子化パラメータのオーバーヘッドを導入する。コストは以下の通り：
  > *M<sub>kv</sub>* = (*G* · *T* · *n<sub>layers</sub>* · *n<sub>kv_heads</sub>* · *d<sub>head</sub>* · 2) · [*P<sub>kv</sub>*/8 + (1/*g<sub>kv</sub>*) · (*P<sub>S</sub>* + *P<sub>Z</sub>*)/8]
  
  ここで*g<sub>kv</sub>*はグループサイズ、*P<sub>S</sub>*と*P<sub>Z</sub>*はスケールとゼロポイントの精度。我々の実験では、対称量子化（*P<sub>Z</sub>* = 0）を*g<sub>kv</sub>* = 64、*P<sub>S</sub>* = 16で使用し、*P<sub>kv</sub>* ∈ {8, 4, 2}をテストする。

以下は評価モデルのアーキテクチャの詳細とトークンあたりのKVキャッシュサイズである。

### 表2：アーキテクチャ仕様とトークンあたりのKVキャッシュメモリ

| モデル | *n<sub>layers</sub>* | *n<sub>kv_heads</sub>* | *d<sub>head</sub>* | KVキャッシュ (KB/トークン) |
|---|---|---|---|---|
| Qwen3-0.6B | 28 | 8 | 128 | 112 |
| Qwen3-1.7B | 28 | 8 | 128 | 112 |
| Qwen3-4B | 36 | 8 | 128 | 144 |
| Qwen3-8B | 36 | 8 | 128 | 144 |
| Qwen3-14B | 40 | 8 | 128 | 160 |
| Qwen3-32B | 64 | 8 | 128 | 256 |

---

## 付録B 並列スケーリングの詳細結果

図10はセクション4で議論した並列スケーリング分析のモデルごとのプロットを示す。

> **図10：並列スケーリングのモデルごとのメモリ対精度（AIME25）。** 各プロットは、単一のモデルと重み精度に対するメモリ・精度トレードオフを示し、シリアルスケーリング（*G* = 1）と*G* ∈ {1, 3, 4, 6, 8, 12, 16}のサンプリンググループサイズを増加させる並列スケーリングを比較する。各曲線に沿った点はバジェットフォーシングによるトークンバジェットの増加を表す。並列スケーリングは有効サイズが8ビット4Bより大きいモデルに対してのみメモリ・精度トレードオフを改善する。

---

## 付録C KVキャッシュ圧縮の詳細結果

図11と図12はセクション5で議論したKVキャッシュ圧縮分析のモデルごとの結果を示す。退避については、StreamingLLMの結果も提示する。ここでは、与えられた保持バジェット*T<sub>retain</sub>*に対して最初の*T<sub>retain</sub>*/2トークンと最新の*T<sub>retain</sub>*/2トークンを保持する。

> **図11：KVキャッシュ戦略別のモデルごとのメモリ対精度（AIME25、4B超のモデル）。** 各プロットは、単一のモデルと重み精度に対するメモリ・精度トレードオフを示し、KVキャッシュ退避手法（R-KV、StreamingLLM）とKVキャッシュ量子化および無圧縮を比較する。各曲線に沿った点は処理トークン数の増加を表す。

> **図12：KVキャッシュ戦略別のモデルごとのメモリ対精度（AIME25、4B以下のモデル）。** 各プロットは、単一のモデルと重み精度に対するメモリ・精度トレードオフを示し、KVキャッシュ退避手法（R-KV、StreamingLLM）とKVキャッシュ量子化および無圧縮を比較する。各曲線に沿った点は処理トークン数の増加を表す。

---

## 参考文献

- AIME. AIME Problems and Solutions. https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions, 2025.
- Allen-Zhu, Z. and Li, Y. Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv:2404.05405, 2024.
- Badri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, 2023.
- Brown, B. et al. Large language monkeys: Scaling inference compute with repeated sampling. arXiv:2407.21787, 2024.
- Cai, Z. et al. R-kv: Redundancy-aware kv cache compression for training-free reasoning models acceleration. arXiv:2505.24133, 2025.
- Chee, J. et al. Quip: 2-bit quantization of large language models with guarantees. NeurIPS, 2023.
- Comanici, G. et al. Gemini 2.5. arXiv:2507.06261, 2025.
- Dao, T. Flashattention-2. arXiv:2307.08691, 2023.
- Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. ICML, 2023.
- Dettmers, T. et al. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. NeurIPS, 2022.
- Feng, G. et al. How numerical precision affects arithmetical reasoning capabilities of LLMs. ACL Findings, 2025.
- Frantar, E. et al. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv:2210.17323, 2022.
- Gadre, S.Y. et al. Language models scale reliably with over-training and on downstream tasks. ICLR, 2025.
- Ge, S. et al. Model tells you what to discard: Adaptive kv cache compression for LLMs. arXiv:2310.01801, 2023.
- Guo, D. et al. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv:2501.12948, 2025.
- Henighan, T. et al. Scaling laws for autoregressive generative modeling. arXiv:2010.14701, 2020.
- Hoffmann, J. et al. Training compute-optimal large language models. arXiv:2203.15556, 2022.
- Hooper, C. et al. KVQuant: Towards 10 million context length LLM inference with KV cache quantization. NeurIPS, 2024.
- Jaech, A. et al. OpenAI o1 system card. arXiv:2412.16720, 2024.
- Kang, H. et al. GEAR: An efficient KV cache compression recipe. arXiv:2403.05527, 2024.
- Kaplan, J. et al. Scaling laws for neural language models. arXiv:2001.08361, 2020.
- Kim, J. et al. Lexico: Extreme KV cache compression via sparse coding over universal dictionaries. ICML, 2024.
- Kim, S. et al. SqueezeLLM: Dense-and-sparse quantization. arXiv:2306.07629, 2023.
- Kimi Team. Kimi k1.5: Scaling reinforcement learning with LLMs. arXiv:2501.12599, 2025.
- Kumar, T. et al. Scaling laws for precision. arXiv:2411.04330, 2024.
- Kurtić, E. et al. Deployment-ready reasoning with quantized DeepSeek-R1 models. 2025.
- Kwon, W. et al. Efficient memory management for large language model serving with PagedAttention. SOSP, 2023.
- Li, Y. et al. SnapKV: LLM knows what you are looking for before generation. NeurIPS, 2024.
- Li, Z. et al. From system 1 to system 2: A survey of reasoning large language models. arXiv:2502.17419, 2025.
- Lin, J. et al. AWQ: Activation-aware weight quantization. MLSys, 2024.
- Liu, L. et al. FlashRL: 8bit rollouts, full power RL. 2025a.
- Liu, R. et al. Quantization hurts reasoning? An empirical study on quantized reasoning models. arXiv:2504.04823, 2025b.
- Liu, Z. et al. LLM-QAT: Data-free quantization aware training for large language models. arXiv:2305.17888, 2023a.
- Liu, Z. et al. ParetoQ: Scaling laws in extremely low-bit LLM quantization. arXiv:2502.02631, 2025c.
- Liu, Z. et al. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression. NeurIPS, 2023b.
- Liu, Z. et al. KIVI: A tuning-free asymmetric 2bit quantization for KV cache. arXiv:2402.02750, 2024.
- Ma, S. et al. The era of 1-bit LLMs: All large language models are in 1.58 bits. arXiv:2402.17764, 2024.
- Mekala, A. et al. Does quantization affect models' performance on long-context tasks? arXiv:2505.20276, 2025.
- Morris, J.X. et al. How much do language models memorize? arXiv:2505.24832, 2025.
- Muennighoff, N. et al. s1: Simple test-time scaling. arXiv:2501.19393, 2025.
- Qwen Team. QwQ-32B: Embracing the power of reinforcement learning. 2025.
- Rein, D. et al. GPQA: A graduate-level google-proof Q&A benchmark. CoLM, 2024.
- Sadhukhan, R. et al. Kinetics: Rethinking test-time scaling laws. arXiv:2506.05333, 2025.
- Snell, C. et al. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv:2408.03314, 2024.
- Wang, J. et al. Scaling over scaling: Exploring test-time scaling Pareto in large reasoning models. arXiv:2505.20522, 2025.
- Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. arXiv:2203.11171, 2022.
- Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.
- Wu, Y. et al. Inference scaling laws: An empirical analysis of compute-optimal inference. arXiv:2408.00724, 2024.
- Xiao, G. et al. SmoothQuant: Accurate and efficient post-training quantization for large language models. ICML, 2023a.
- Xiao, G. et al. Efficient streaming language models with attention sinks. arXiv:2309.17453, 2023b.
- Yang, A. et al. Qwen3 technical report. arXiv:2505.09388, 2025.
- Yao, Z. et al. ZeroQuant-v2. arXiv:2303.08302, 2023.
- Zhang, Z. et al. H2O: Heavy-hitter oracle for efficient generative inference of large language models. NeurIPS, 2023.
- Zhao, J.X. et al. Test-time scaling in reasoning models is not effective for knowledge-intensive tasks yet. arXiv:2509.06861, 2025.
